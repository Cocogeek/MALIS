{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MALIS Lab Session 1 - Fall 2018</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to practice with Neural Networks (Multi-Layer Perceptrons) via simple classification experiments and the (partial) implementation of the feedforward and backpropagation procedures. For this lab, the implementation of the MLP simulator is in Python 3.\n",
    "\n",
    "Experiments should be made by groups of two students. Each group should produce a Jupyter Notebook with all their results and comments. We strongly encourage the addition of plots and visual representation to the report, bearing in mind that comments on the graphical data are still necessary. Code for adding images to your notebook: ```<img src=\"path/to/image.png\" />```.\n",
    "\n",
    "Submit your complete notebook as an archive (tar -cf groupXnotebook.tar lab1/) . Deadline for submitting your notebook: 30 November 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "There are three parts to this lab session. \n",
    "\n",
    "1. A \"theoretical\" part: Given a set of training examples you have to decide on the architecture of the feed-forward neural network such as; number of layers, number of neuron per layers and finally the values of the weights. \n",
    "\n",
    "2. A \"programming\" part: Given the skeleton of the Python code of an MLP simulator, implement the missing functions (feedforward and backpropagation procedures). \n",
    "\n",
    "3. An \"experimental\" part: Having completed the implementation of the MLP simulator, the final step consist on training the network and testing its accuracy.\n",
    "\n",
    "<h2>Part 1: Design a neural network</h2>\n",
    "The aim of this part is to get a better understanding of the basics of Neural Networks construction. A number of sample points on a 128 by 128 grid have been assigned one out of three colors (red, green or blue). You should build a Neural Network with two inputs and three outputs which provides the exact coloring for these points. The problem can be visualized in the following figure: \n",
    "\n",
    "<img src=\"data_set.jpg\" />\n",
    "\n",
    "The file set30.x1x2rgb (in .\\data\\) contains the data corresponding to the problem defined above. The visual representation of the problem (above figure) is stored in data_set.jpg.\n",
    "\n",
    "The problem:\n",
    "\n",
    "Pairs of x1 and x2 coordinates (both ranging between 0 and 127) are associated with a specific color: \n",
    "\n",
    "* Red: output 1 0 0, \n",
    "* Green: output 0 1 0, \n",
    "* Blue: output 0 0 1. \n",
    "\n",
    "The objective of the network is to correctly determine for any given (x1, x2) coordinate pair the corresponding color. \n",
    "Your task is to <b>manually define a Neural Network which performs this task perfectly (do not forget to justify your answer)</b>. There is no need for programming or iterative training. The transfer function is assumed to be the step function: \n",
    "\n",
    "$f(t) = (t > 0)$ (it is equal to 1 if t is positive, 0 otherwise). \n",
    "\n",
    "Of course, it is your task to define the number of layers, the number of neurons per layer, and the exact values for the weights. \n",
    "\n",
    "<i>Hint: You may remember the XOR problem and how it was solved.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data are linearly separable, so we can define classifiers without any errors. <br/>\n",
    "\n",
    "For the design of the neural network, we need :<br/>\n",
    "       -<b>2 input neurons</b> because for each point we have exactly <b>2 coordinates</b><br/>\n",
    "       -<b>3 output neurons</b> because we have <b>3 different outputs</b> (red,green,blue)<br/>\n",
    "       -<b>1 hidden layer of 2 neurons</b> because it is a <b>linearly separable case</b>, and each neuron represent an <b>hyperplane</b>.<br/>\n",
    "\n",
    "The hidden layer represents the classifiers. The outputs of the hidden layer are 0 or 1 for each neuron. <br/>\n",
    "The output layer represents logical functions, determined thanks to a chart of truth. Indeed, the combination of the output of the hidden layer allows to determine the color for each point.\n",
    "\n",
    "<img src=\"classifier.png\" />\n",
    "<img src=\"table.png\" />\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input layer:  2 units,  x1   x2\n",
    "First hidden layer:\n",
    "    2 neurons:\n",
    "    neuron 1: w11 = <0.64>           f1: x2 = -w11*x1 + b\n",
    "                  w21 = <1>\n",
    "                  b1  = <80>\n",
    "    neuron 2: w12 = <-1.3>           f2: x2= -w11*x1 +b\n",
    "                  w22 = <1>\n",
    "                  b2  = <-18.3>    \n",
    "\n",
    "output layer:\n",
    "    3 neurons:\n",
    "    neuron 1: w11 = <-1>          NOT(x1)\n",
    "                  b1  = <-0.5>\n",
    "    neuron 2: w12 = <1>           x1 AND x2\n",
    "                  w22 = <1>\n",
    "                  b2  = <1.5>\n",
    "    neuron 3: w13 = <1>          x1 AND NOT(x2)\n",
    "                  w23 = <-1>\n",
    "                  b3  = <0.5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Implementation of the MLP simulator</h2>\n",
    "The task here is to implement the missing parts of a code written to simulate multi-layer perceptrons. The code can be found in your directory under the filename utils.py (but you will not edit that file, all your code will be written in your notebook). Here is a brief explanation about the MLP simulator: \n",
    "\n",
    "A network description file has to be provided. This is a text file which contains information about the number of layers in the network and the number of units (neuron) for each layer. Here is an example of such a file: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example describes a 2 layer network with 2 hidden units and 3 output units. \n",
    "Additionally a pattern (or example set) file has to be provided. This file contains a number of example pattern with input and output values. For an example of such a file look at ./data/set30.x1x2rgb.\n",
    "\n",
    "As you know, transfer functions of an MLP need to be differentiable to train it. Therefore, we replace the step function by a sigmoid function.\n",
    "\n",
    "Now that you have a broad overview of the program your task is to <b>implement the feedforward function of the Neuron class</b>. Obviously, you can find help in the notes from the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell to import relevant classes and functions\n",
    "from utils import Neuron, Dataset, Layer, MLP, sigmoid, d_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 neuron contains its inputs and the associated weights\n",
    "def feedforward(self):   #self is a neuron here\n",
    "    res = 0. # Contains the weighted sum of the inputs of the neuron\n",
    "    for i in range(len(self.inputs)):   #go through all the inputs\n",
    "        res += self.inputs[i]*self.weights[i]### IMPLEMENTATION REQUIRED ### #implements input*weight\n",
    "    res += self.bias  #add the threshold of the neuron\n",
    "    self.u = res   #activation of the neuron\n",
    "    self.out = sigmoid(res)  #output of the neuron\n",
    "\n",
    "Neuron.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the Backpropagation function, <b>write the recursive formula for the partial derivative of the error with respect to the activation (neuron j of layer i) as a function of the weights and partial derivative of the error in layer i+1 from the course material</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial u^{(i)}_j}=  \\sum_k \\frac{\\partial L}{\\partial u^{(i-1)}_k}  * w^{(i-1)}_{jk} *f^{'}{(u^{i}_j)}     $$\n",
    "\n",
    "i: layer<br/>\n",
    "j: neuron <br/>\n",
    "k: input dans j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>implement the compute_gradients() and the apply_gradient() functions of the MLP class</b>.\n",
    "\n",
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(self):   #self is a Multi-Layer Perceptron, the layers start from layer N to layer 0\n",
    "    # First compute derivatives for the last layer\n",
    "    layer = self.layers[-1]  #we focus on the last layer\n",
    "    for i in range(len(layer)):  #i points to a neuron of the last layer\n",
    "        # Compute dL/du_i\n",
    "        neuron = layer.neurons[i]    #neuron is a neuron of the last layer\n",
    "        o = neuron.out    #output of the neuron\n",
    "        u = neuron.u      #activation of the neuron\n",
    "        t = self.gt[i]    #expected output of the neuron of this layer\n",
    "        neuron.d_u = 2*(o-t)*d_sigmoid(u) ### IMPLEMENTATION REQUIRED ###  #derivate of the activation\n",
    "        for j in range(len(neuron.weights)):  #j points to an input of neuron, which are the ouptut of neurons from the previous layer\n",
    "            # Compute dL/dw_ji\n",
    "            neuron.d_weights[j] = neuron.d_u * neuron.inputs[j] ### IMPLEMENTATION REQUIRED ### #derivate of the weght\n",
    "\n",
    "    # Then compute derivatives for other layers\n",
    "    for l in range(2, len(self.layers)):  # l  points to a layer, go back to the beginning of Network\n",
    "        layer = self.layers[-l]           #layer is the lth layer, startig from the end\n",
    "        next_layer = self.layers[-l+1]    #next_layer is the layer on the right (not the previous one)\n",
    "        for i in range(len(layer)):       #i points to the neurons of the layer\n",
    "            # Compute dL/du_i\n",
    "            neuron = layer.neurons[i]\n",
    "            d_u = 0.\n",
    "            u = neuron.u\n",
    "            for j in range(len(next_layer)):  #j points to the neuron of  next layer\n",
    "                next_neuron = next_layer.neurons[j]\n",
    "                d_u += next_neuron.d_u*next_neuron.weights[i]### IMPLEMENTATION REQUIRED ### #compute sum\n",
    "            neuron.d_u= d_u *d_sigmoid(u)\n",
    "            for j in range(len(neuron.weights)):#j points to an input of the layer, output of the previsous layer (which is next layer)\n",
    "                # Compute dL/dw_ji\n",
    "                neuron.d_weights[j] = neuron.d_u * neuron.inputs[j] ### IMPLEMENTATION REQUIRED ###\n",
    "\n",
    "def apply_gradients(self, learning_rate):\n",
    "    # Change weights according to computed gradients\n",
    "    for i in range(1, len(self.layers)):  #i points to a layer\n",
    "        layer = self.layers[i]\n",
    "        for j in range(1, len(layer)):   #j points to a neuron of the layers\n",
    "            neuron = layer.neurons[j]\n",
    "            for k in range(len(neuron.d_weights)):   #k points to the input of the neuron\n",
    "                neuron.weights[k] -= learning_rate * neuron.d_weights[k]### IMPLEMENTATION REQUIRED ###\n",
    "            neuron.bias -= learning_rate * neuron.bias ### IMPLEMENTATION REQUIRED ###\n",
    "\n",
    "MLP.compute_gradients = compute_gradients\n",
    "MLP.apply_gradients = apply_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Training and Accuracy experiments</h2>\n",
    "\n",
    "Train the network on the problem stated in Part 1, using the training set set120.x1x2rgb and the following parameters:\n",
    "* learning rate: 2.0; \n",
    "* number of training cycles: 1000\n",
    "\n",
    "In order to do so you will need to create a network definition file (as described in the introduction) containing the details of the network architecture. \n",
    "Evaluate the accuracy using set30.x1x2rgb as the test set (you can use the setdataset() function of the MLP class to change between training and test sets).\n",
    "\n",
    "Experiment with the learning rate and the number of training cycles. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Result on test data ===\n",
      "Accuracy: 33.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "# This is an example code that you can adjust to your liking\n",
    "\n",
    "train_datafile = \"data/set120.x1x2rgb\"\n",
    "train_data = Dataset(train_datafile)\n",
    "\n",
    "test_datafile = \"data/set30.x1x2rgb\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "nnfile = \"data/NN.dat\"\n",
    "mlp = MLP(nnfile, train_data, print_step=100, verbose=False)\n",
    "\n",
    "mlp.train(1000, 1.0)\n",
    "mlp.make_plot()\n",
    "\n",
    "print(\"=== Result on test data ===\")\n",
    "mlp.setdataset(test_data)\n",
    "mlp.print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your comments</h3>\n",
    "When we increase the number of training cycles to a certain point, the accuracy on test data increases and the accuracy on train data decreases. At that point, the model fits the test data which is good.\n",
    "However, upon this certain point, the result is very bad on test data : we observe overfitting (the model is too precise for a given sample).\n",
    "We can note also, when this number is low, the accuracy of the train and test data are pretty similar.\n",
    "\n",
    "number of training cycles = 1000;learning rate=2.0 \n",
    "<img src=\"traindatanbercycles1000.png\" />\n",
    "<img src=\"testdatanbercycles1000.png\" /> \n",
    "number of training cycles = 5000;learning rate=2.0 \n",
    "<img src=\"traindatanbercycles5000.png\" />\n",
    "<img src=\"testdatanbercycles5000.png\" />\n",
    "number of training cycles = 10000;learning rate=2.0 \n",
    "<img src=\"train10000.png\" />\n",
    "<img src=\"test10000.png\" />\n",
    "\n",
    "When we decrease the learning rate, we see that the result is better. When the learning rate is low, the result varies (the accuracy can be good 60% or bad 30%). Indeed, the algorithm doesn't have time to reach the minimum point whith a given number of cycles so the result is bad, when it has time, then the result is good. When the learning rate is higher, the results are more stable and quite good. NB : we can't go over 2, maybe because after that point, it's possible that we miss the local minima.\n",
    "\n",
    "number of training cycles = 1000;learning rate=1.5\n",
    "<img src=\"train1.5.png\" /> <img src=\"test1.5.png\" />\n",
    "number of training cycles = 1000;learning rate=1.0 \n",
    "<img src=\"trainlr1.png\" /> <img src=\"testlr1.png\" />\n",
    "number of training cycles = 1000;learning rate=0.5 \n",
    "<img src=\"trainlr0.5.png\" /> <img src=\"testlr0.5.png\" />\n",
    "\n",
    "To conclude, to have a good result, we need to find the good trade-off (high but not too high) for the number of training cycles and the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
